{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language as a small world network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call a network small-world network `[1]`, if its nodes are accessible in just a few steps from any other node in the network, while its clustering coefficient (the franction of closed triangles out of all possible triads) remains high. The accessibility in other words means, that the networks' diameter $d$ - the longest shortest path between two nodes - grows only logarithmically with the size of the network $N$. Perhaps the most famous example of this small-word phenomenon is the [six-degrees of separation](https://en.wikipedia.org/wiki/Six_degrees_of_separation) experiment of Milgram, in which he argued that on average, every single man on Earth is accessible by another man through personal connections in just six steps.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/88/Six_degrees_of_separation_01.png)\n",
    "\n",
    "While different theoretical network models, such as the [Watts-Strogatz model](https://en.wikipedia.org/wiki/Watts%E2%80%93Strogatz_model), the [Barabasi-Albert model](https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model) and the [Erdos-Renyi model](https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model) all exhibit this small-world property [1,2], many real-word networks also show this phenomenon. This is a very important finding, since the small diameter influences the behaviour of dynamical processes that happen along the nodes or edges of these networks, such as the spread of diseases, computer viruses or social influence. Therefore, investigating real networks and their topology has recently been a very popular topic, and it also became more and more accessible as we have many digital network datasets at hand.\n",
    "\n",
    "## Language networks\n",
    "\n",
    "While representing social connections or power grids as networks might come naturally, other fields also benefit from the somewhat less intuitive network representation of their data. Such a counterintuitive representation is the network of words in linguistics. Because networks are useful when encoding interactions, the network of co-occurring words can be though of as a network of close concepts. In the following, you are going to try to reproduce the findings of an article `[3]` that investigated the network of the English language for the [British National Corpus](http://www.natcorp.ox.ac.uk/) in 2000. (A corpus is a collection of texts in linguistic analysis.) Such a corpus is very diverse in its nature, and contains both written and spoken texts. But how does the language of an 'average Joe' look like? We can get closer to the answer, if we create a corpus based on social media posts. By using a freely available API to collect messages from a social media platform of your choice (e.g. Twitter, Reddit), you can compare the structure of social media language network to that of the British National Corpus. *Disclaimer: the author of the project does not know the outcome, it is a completely open-ended research project!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "## 1. Data acquisition and preparation\n",
    "\n",
    "Use the API of your chosen social media platform to download English-language messages. The easiest way is to select an area from an English-speaking country (US, UK have both high tweet densities, which is done by giving bounding box coordinates to the streaming query) or an english subreddit. You have to create a profile (if you already did not have one), and an application with credentials for for the authorization of the downloading. It is helpful to use a python library such as `tweepy` or `twython` for Twitter, or `praw` for Reddit that have built-in routines for the download. However, you may use whatever tool you'd like.\n",
    "\n",
    "Have a look at the downloaded data. Try to find methods to include only sensible messages, e.g. exclude retweets from you dataset, or reddit posts with only image content, etc... Then have a look at the texts, which is what you'll need for this task. It contains not only words, but also mentions of usernames beginning with a `@` sign, and hashtags marked by `#`, and URLs for example. Break the sentences into words, clean the text from emojis, punctuation etc., then use regular expressions and tokenizers to remove the special parts. The creation of words is called *tokenization* in the natural language processing literature, and there are very good tokenizers specialized for tweets (which could also work on other posts), search for one of them! Try to stem the words to make 'class' and 'classes' fall into one category. You can use the Snowball-stemmer of the `nltk` library, for example. *For examples and tips, you can refer to the NLP lecture notes from the Data exploration and visualization class of the Spring term.*\n",
    "\n",
    "## 2. Creating the language network\n",
    "\n",
    "If you have mapped your downloaded data to sensible word sequences, create the network of words by linking those words that co-occured in a tweet/post. Create a weighted network, where link weights correspond to the number of times words co-occured in the dataset.\n",
    "\n",
    "## 3. Investigating the language network\n",
    "\n",
    "Take the two main conclusions from the paper `[3]`: that the language network is small-word and that the degree distribution follows a scale-free distribution. Reproduce the three figures of the article on your own dataset. Try to differentiate between the 'kernel lexicon' and the 'unlimited lexicon' in your own network. Can you find such a distinction? What are you numerical results compared to that of the authors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "`[1]` Watts, D. J. & Strogatz, S. H. Collective dynamics of ‘small-world’ networks. Nature 393, 440–2 (1998).\n",
    "\n",
    "`[2]` [Chapter 3.8](http://networksciencebook.com/chapter/3#small-worlds) from Barabási, A.-L. & Pósfai, M. Network Science. (Cambridge University Press, 2016). \n",
    "\n",
    "`[3]` Cancho, R. F. i. & Sole, R. V. The small world of human language. Proc. R. Soc. B Biol. Sci. 268, 2261–2265 (2001)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
