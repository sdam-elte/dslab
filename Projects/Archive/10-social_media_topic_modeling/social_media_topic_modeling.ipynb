{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community finding alogrithms on a language network\n",
    "\n",
    "If we consider our own social connections, it is evident that there is clustering in it to a very high extent. We went to school with a certain circle of friends, who are likely to know each other, we work at a company, where people also know each other, but these two groups do not necessarily overlap. Many other real-word networks exhibit a high degree of clustering. But finding these communities is not a trivial task, and there are many approaches in the literature for community-finding algorithms (for details see the excellent review paper of Santo Fortunato `[1]`).\n",
    "\n",
    "![](http://snap.stanford.edu/agm/agm_g.png)\n",
    "\n",
    "While representing social connections or power grids as networks might come naturally, other fields also benefit from the somewhat less intuitive network representation of their data. Such a counterintuitive representation is the network of words in linguistics. Because networks are useful when encoding interactions, the network of co-occurring words can be though of as a network of close concepts. In these language networks, community finding corresponds to finding related concepts, or groups of words, which can be attributed to a certain topic.\n",
    "\n",
    "In a recent paper `[2]`, the authors argue that a network-based approach to finding topics in texts is superior to previous probabilistic methods. Because it is a very demanding task only to understand the methods and concepts of the paper, your main task will be to understand the two approaches discussed in the article, find libraries that implement them, and collect some data from a social media platform (e.g. Twitter or Reddit) on which you can test the results. Topic modeling on posts and comments can be a very tedious task due to the shortness and nature of the texts, so if you are only able to retrieve partly meaningful results, don't be discouraged. *Disclaimer: the author of the project does not know the outcome, it is a completely open-ended research project!*\n",
    "\n",
    "# Tasks\n",
    "\n",
    "## 1. Data acquisition and preparation\n",
    "\n",
    "Use the API of your chosen platform to download English-language messages related to a certain event or discussion. The easiest way is to select popular hashtag or subreddit that attracts many users to talk about (e.g. vaccination, Russian invasion of Ukraine, energy crisis, Will Smith slap, etc.). You have to create a profile (if you already did not have one), and an application with credentials for for the authorization of the downloading. It is helpful to use a python library such as `tweepy` or `twython` for Twitter, or `praw` for Reddit that have built-in routines for the download. However, you may use whatever tool you'd like.\n",
    "\n",
    "Have a look at the downloaded data. Try to find methods to include only sensible messages, e.g. exclude retweets from you dataset, or reddit posts with only image content, etc... Then have a look at the texts, which is what you'll need for this task. It contains not only words, but also mentions of usernames beginning with a `@` sign, and hashtags marked by `#`, and URLs for example. Break the sentences into words, clean the text from emojis, punctuation etc., then use regular expressions and tokenizers to remove the special parts. The creation of words is called *tokenization* in the natural language processing literature, and there are very good tokenizers specialized for tweets (which could also work on other posts), search for one of them! Try to stem the words to make 'class' and 'classes' fall into one category. You can use the Snowball-stemmer of the `nltk` library, for example. *For examples and tips, you can refer to the NLP lecture notes from the Data exploration and visualization class of the Spring term.*\n",
    "\n",
    "## 2. Creating the language network\n",
    "\n",
    "If you have mapped your data to sensible word sequences, create the network of words by linking those words that co-occured in the tweet/post. Create a weighted network, where link weights correspond to the number of times words co-occured in the dataset.\n",
    "\n",
    "## Topic modeling\n",
    "\n",
    "Explore the proposed methods (plsa, lda, hsbm) from the article on you dataset!  Do you get meaningful results? If yes, what parameter setting lead to it? Present and discuss the topics you got. If not, what could be the problem? How do other community finding algorithms perform?\n",
    "\n",
    "## References \n",
    "\n",
    "`[1]` Fortunato, S. Community detection in graphs. (2009). doi:10.1016/j.physrep.2009.11.002\n",
    "\n",
    "`[2]`  Gerlach, M., Peixoto, T. P. & Altmann, E. G. A network approach to topic models. Sci. Adv. 4, (2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
